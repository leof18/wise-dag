{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx   \n",
    "from ipywidgets import interact, IntSlider\n",
    "from collections import defaultdict\n",
    "from hierarchy_functions import get_unique_coded_terms, create_SNOMED_CT_graph_based_on_terms, find_lca_and_distance, compute_lcas_and_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DAG Hierarchy\n",
    "\n",
    "**Goal: Group terms and find common name to create a hierarchy of DAG nodes from broad down to specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data\n",
    "1. `dag_df` - Standardized DAG dataframe from workshop\n",
    "2. `concept_df` - Athena concept dataframe containing all the athena ids, concept codes, and concept names\n",
    "3. `concept_relationship_df` - Athena dataframe containing the relationships between all the concepts\n",
    "4. `concept_ancestor_df` - Athena dataframe containing information on the ancestors of terms (unfortunately seems incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_df = pd.read_csv('../data/DAGs_standardized.csv', dtype={'Exposure': str, 'Outcome':str})\n",
    "concept_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT.csv', sep='\\t', dtype={'concept_code': str, 'concept_id': str}, low_memory=False)\n",
    "concept_relationship_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT_RELATIONSHIP.csv', sep='\\t', dtype={'concept_id_1':str, 'concept_id_2': str}, low_memory=False)\n",
    "\n",
    "concept_df = concept_df[concept_df['vocabulary_id'] == 'SNOMED']\n",
    "concept_relationship_df = concept_relationship_df[\n",
    "    (concept_relationship_df['concept_id_1'].isin(concept_df['concept_id'])) &\n",
    "    (concept_relationship_df['concept_id_2'].isin(concept_df['concept_id']))]\n",
    "\n",
    "custom_concepts = pd.read_excel('../Standardization/custom_snomed_concepts_mapping.xlsx', sheet_name='concepts', dtype={'concept_id':str})\n",
    "custom_relationships = pd.read_excel('../Standardization/custom_snomed_concepts_mapping.xlsx', sheet_name='relationships', dtype={'concept_id_1':str, 'concept_id_2':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding concept class id to custom concepts based on their mapped parent relationship concept class id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_concepts = custom_concepts.merge(\n",
    "    custom_relationships.merge(concept_df, left_on='concept_id_1', right_on='concept_id')[['concept_id_2','concept_class_id']],\n",
    "    left_on='concept_id', right_on='concept_id_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add custom concepts and relationships to concept and relationships database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['concept_id','concept_name','concept_class_id']\n",
    "concept_df = pd.concat([concept_df[cols1], custom_concepts[cols1]]).reset_index(drop=True)\n",
    "\n",
    "cols2 = ['concept_id_1', 'concept_id_2', 'relationship_id']\n",
    "concept_relationship_df = pd.concat([concept_relationship_df[cols2], custom_relationships[cols2]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv('../data/all_snomed_concepts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_id_1</th>\n",
       "      <th>concept_id_2</th>\n",
       "      <th>relationship_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4285289</td>\n",
       "      <td>4008238</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4285289</td>\n",
       "      <td>4047782</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4285289</td>\n",
       "      <td>4120132</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4285289</td>\n",
       "      <td>4264018</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4285289</td>\n",
       "      <td>44783547</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116951</th>\n",
       "      <td>441840</td>\n",
       "      <td>12</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116952</th>\n",
       "      <td>4019381</td>\n",
       "      <td>13</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116953</th>\n",
       "      <td>441840</td>\n",
       "      <td>14</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116954</th>\n",
       "      <td>4011630</td>\n",
       "      <td>15</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7116955</th>\n",
       "      <td>4295659</td>\n",
       "      <td>16</td>\n",
       "      <td>Subsumes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>762612 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        concept_id_1 concept_id_2 relationship_id\n",
       "11           4285289      4008238        Subsumes\n",
       "17           4285289      4047782        Subsumes\n",
       "42           4285289      4120132        Subsumes\n",
       "74           4285289      4264018        Subsumes\n",
       "99           4285289     44783547        Subsumes\n",
       "...              ...          ...             ...\n",
       "7116951       441840           12        Subsumes\n",
       "7116952      4019381           13        Subsumes\n",
       "7116953       441840           14        Subsumes\n",
       "7116954      4011630           15        Subsumes\n",
       "7116955      4295659           16        Subsumes\n",
       "\n",
       "[762612 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_relationship_df[concept_relationship_df.relationship_id=='Subsumes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get set of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_terms = get_unique_coded_terms(dag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary to convert codes to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_name = dict(zip(concept_df[\"concept_id\"], concept_df[\"concept_name\"]))\n",
    "name_to_code = dict(zip(concept_df[\"concept_name\"], concept_df[\"concept_id\"]))\n",
    "\n",
    "my_terms_written = [code_to_name[code] for code in my_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to original name map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_original_name = pd.read_csv('..\\Standardization\\complete_athena_id_mapping.csv', dtype={'target_concept_id': str}).groupby('target_concept_id').agg(list).to_dict()['source_code_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_relationship_ids = [\"Subsumes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchy(my_terms_filtered, G_clinical, compute_lcas_and_distances, find_lca_and_distance):\n",
    "    \"\"\"\n",
    "    Build a hierarchy from terms using a graph, precomputing LCAs and distances.\n",
    "\n",
    "    Parameters:\n",
    "    - my_terms_filtered (set): A set of terms to process.\n",
    "    - G_clinical (networkx.Graph): The graph representing the relationships.\n",
    "    - compute_lcas_and_distances (function): A function that computes LCAs and distances for initial terms.\n",
    "    - find_lca_and_distance (function): A function to compute the LCA and distance for a pair of terms.\n",
    "\n",
    "    Returns:\n",
    "    - hierarchy (defaultdict): The hierarchy built from the terms.\n",
    "    \"\"\"\n",
    "    # Precompute LCAs and distances\n",
    "    lca_distances = compute_lcas_and_distances(my_terms_filtered, G_clinical)\n",
    "\n",
    "    remaining_terms = my_terms_filtered.copy()\n",
    "    hierarchy = defaultdict(list)\n",
    "    processed_pairs = set()  # Track processed pairs to avoid infinite loops\n",
    "\n",
    "    def has_descendants(term, remaining_terms, graph):\n",
    "        \"\"\"Check if a term has descendants in the remaining terms.\"\"\"\n",
    "        descendants = nx.descendants(graph, term)\n",
    "        return bool(descendants & set(remaining_terms))\n",
    "    \n",
    "    # Special case: If there is only one node, add it to the hierarchy and return\n",
    "    if len(remaining_terms) == 1:\n",
    "        single_node = next(iter(remaining_terms))  # Get the single node\n",
    "        hierarchy[single_node] = []  # No children\n",
    "        return hierarchy\n",
    "\n",
    "    while len(remaining_terms) > 1:\n",
    "        shortest_distance = float(\"inf\")\n",
    "        best_pair = None\n",
    "        best_lca = None\n",
    "\n",
    "        # Find the best pair of terms based on distance\n",
    "        for node1 in sorted(remaining_terms):\n",
    "            for node2 in sorted(remaining_terms):\n",
    "                if node1 >= node2: # skip duplicates or reverse pairs\n",
    "                    continue\n",
    "                key = frozenset({node1, node2})\n",
    "                if key in processed_pairs:\n",
    "                    continue\n",
    "                if key not in lca_distances[1]:\n",
    "                    lca, distance = find_lca_and_distance(G_clinical, node1, node2)\n",
    "                    lca_distances[0][key] = lca\n",
    "                    lca_distances[1][key] = distance\n",
    "                dist = lca_distances[1][key]\n",
    "\n",
    "                # Update the best pair if this distance is shorter\n",
    "                if dist < shortest_distance or (dist == shortest_distance and (node1, node2) < best_pair): # tie break to prevent random result\n",
    "                    shortest_distance = dist\n",
    "                    best_pair = (node1, node2)\n",
    "                    best_lca = lca_distances[0][key]\n",
    "\n",
    "        if best_pair is None:\n",
    "            break\n",
    "        \n",
    "        hierarchy[best_lca].append(best_pair)\n",
    "        processed_pairs.add(frozenset(best_pair))\n",
    "\n",
    "        # Replace grouped nodes with their LCA if they no longer have descendants\n",
    "        to_remove = set()\n",
    "        to_add = set()\n",
    "\n",
    "        for node in best_pair:\n",
    "            if not has_descendants(node, remaining_terms, G_clinical):\n",
    "                to_remove.add(node)\n",
    "        to_add.add(best_lca)\n",
    "\n",
    "        # Apply changes after the iteration\n",
    "        remaining_terms -= to_remove\n",
    "        remaining_terms |= to_add\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def hierarchy_to_graph(hierarchy, original_terms):\n",
    "    \"\"\"\n",
    "    Convert a hierarchy dictionary to a directed graph, removing duplicates and self-references.\n",
    "    Adds a 'type' attribute to distinguish between original and LCA nodes.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Iterate over the hierarchy\n",
    "    for lca, pairs in hierarchy.items():\n",
    "        # Add the LCA node and mark it as an 'lca'\n",
    "        if lca not in graph:\n",
    "            node_type = \"original\" if lca in original_terms else \"lca\"\n",
    "            graph.add_node(lca, type=node_type)\n",
    "\n",
    "        for pair in pairs:\n",
    "            for node in pair:\n",
    "                # Add the original terms or nodes if not already added\n",
    "                if node not in graph:\n",
    "                    node_type = \"original\" if node in original_terms else \"lca\"\n",
    "                    graph.add_node(node, type=node_type)\n",
    "\n",
    "                # Add edges, avoiding self-references\n",
    "                if lca != node:\n",
    "                    if not graph.has_edge(lca, node):\n",
    "                        graph.add_edge(lca, node, type=\"Subsumes\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def prune_redundant_edges(graph):\n",
    "    \"\"\"Remove redundant edges that create shortcuts in the hierarchy.\"\"\"\n",
    "    edges_to_remove = set()\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        for descendant in descendants:\n",
    "            for intermediate in graph.successors(node):\n",
    "                if intermediate in descendants and graph.has_edge(intermediate, descendant):\n",
    "                    edges_to_remove.add((node, descendant))\n",
    "\n",
    "    graph.remove_edges_from(edges_to_remove)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def replace_nodes_with_names(graph, mapping):\n",
    "    \"\"\"Replace node codes with names based on a mapping dictionary while preserving attributes.\"\"\"\n",
    "\n",
    "    new_graph = nx.DiGraph()\n",
    "\n",
    "    # Add nodes with attributes, replacing codes with names\n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        new_node = mapping.get(node, node)  # Replace code with name if mapping exists\n",
    "        new_graph.add_node(new_node, **attrs)  # Preserve attributes\n",
    "\n",
    "    # Add edges, replacing codes with names\n",
    "    for u, v, attrs in graph.edges(data=True):\n",
    "        new_u = mapping.get(u, u)\n",
    "        new_v = mapping.get(v, v)\n",
    "        new_graph.add_edge(new_u, new_v, **attrs)  # Preserve edge attributes\n",
    "\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Substance',\n",
       " 'Clinical Finding',\n",
       " 'Procedure',\n",
       " 'Observable Entity',\n",
       " 'Context-dependent',\n",
       " 'Morph Abnormality',\n",
       " 'Social Context',\n",
       " 'Event']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_classes = list(concept_df[concept_df.concept_id.isin(my_terms)].concept_class_id.unique())\n",
    "concept_classes.remove('Disorder')\n",
    "concept_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 12 terms to the graph for concept class Substance.\n",
      "\n",
      "Adding 166 terms to the graph for concept class ['Clinical Finding', 'Disorder'].\n",
      "\n",
      "Adding 44 terms to the graph for concept class Procedure.\n",
      "\n",
      "Adding 8 terms to the graph for concept class Observable Entity.\n",
      "\n",
      "Adding 2 terms to the graph for concept class Context-dependent.\n",
      "\n",
      "Adding 13 terms to the graph for concept class Morph Abnormality.\n",
      "\n",
      "Adding 4 terms to the graph for concept class Social Context.\n",
      "\n",
      "Adding 1 terms to the graph for concept class Event.\n",
      "\n",
      "Total number of terms in the Snomed DAG are 250 with 251 edges.\n"
     ]
    }
   ],
   "source": [
    "full_graph = nx.DiGraph()\n",
    "\n",
    "for concept_class in concept_classes:\n",
    "\n",
    "    # Group clinical finding and disorder together\n",
    "    if concept_class=='Clinical Finding':\n",
    "        concept_class = ['Clinical Finding', 'Disorder']\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&\n",
    "                                        (concept_df.concept_class_id.isin(concept_class))].concept_id)\n",
    "\n",
    "    else:\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&\n",
    "                                        (concept_df.concept_class_id==concept_class)].concept_id)\n",
    "\n",
    "    G_x = create_SNOMED_CT_graph_based_on_terms(filtered_terms, concept_relationship_df, selected_relationship_ids)\n",
    "    \n",
    "    hierarchy_x = build_hierarchy(filtered_terms, G_x, compute_lcas_and_distances, find_lca_and_distance)\n",
    "    graph_x = hierarchy_to_graph(hierarchy_x, filtered_terms)\n",
    "    pruned_graph_x = prune_redundant_edges(graph_x)\n",
    "    # graph_with_names_x = replace_nodes_with_names(pruned_graph_x, code_to_name)\n",
    "    \n",
    "    full_graph = nx.compose(full_graph, pruned_graph_x)\n",
    "    # full_graph = nx.compose(full_graph, graph_with_names_x)\n",
    "    print(f'Adding {pruned_graph_x.number_of_nodes()} terms to the graph for concept class {concept_class}.\\n')\n",
    "    # print(f'Adding {graph_with_names_x.number_of_nodes()} terms to the graph for concept class {concept_class}.\\n')\n",
    "print(f'Total number of terms in the Snomed DAG are {full_graph.number_of_nodes()} with {full_graph.number_of_edges()} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [\n",
    "    {\"concept_id_1\": u, \"concept_id_2\": v, **data}\n",
    "    for u, v, data in full_graph.edges(data=True)\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(edges)\n",
    "df.to_csv('../data/graph_relationships.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    {\n",
    "        \"concept_id\": node,\n",
    "        \"type\": data.get(\"type\", None)  # Default to None if missing\n",
    "    }\n",
    "    for node, data in full_graph.nodes(data=True)\n",
    "]\n",
    "\n",
    "nodes_df = pd.DataFrame(nodes)\n",
    "\n",
    "nodes_df = nodes_df.merge(concept_df, how='left', on='concept_id')[['concept_id','concept_name','concept_class_id','type']]\n",
    "nodes_df.to_csv('../data/graph_nodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(full_graph, 'snomed_alternative_grouping_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_depths(graph):\n",
    "    \"\"\"\n",
    "    Precompute the depths (max depth to leaf) for all nodes in the graph.\n",
    "    \"\"\"\n",
    "    depths = {}\n",
    "\n",
    "    def compute_depth(node):\n",
    "        if node in depths:\n",
    "            return depths[node]\n",
    "        if graph.out_degree(node) == 0:  # Leaf node\n",
    "            depths[node] = 0\n",
    "        else:\n",
    "            depths[node] = 1 + max(compute_depth(child) for child in graph.successors(node))\n",
    "        return depths[node]\n",
    "    \n",
    "    for node in nx.topological_sort(graph):\n",
    "        compute_depth(node)\n",
    "    \n",
    "    return depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\wise-dag-ml\\lib\\site-packages\\pygraphviz\\agraph.py:1403: RuntimeWarning: Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a708361904b4d66be7b6de8f601dcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=73), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_reachability_expansion_with_yifan_hu(graph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    \"\"\"\n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true')\n",
    "\n",
    "    # Slider callback function\n",
    "    def update(iteration):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        nodes_to_draw, highlighted_node = iterations[iteration]\n",
    "        \n",
    "        # Collect processed nodes up to the current iteration\n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration)\n",
    "        )\n",
    "        \n",
    "        # Collect blue nodes up to the current iteration\n",
    "        blue_nodes_up_to_now = set(root_nodes)  # Start with root nodes\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            for child in graph.successors(parent_node):\n",
    "                if child not in processed_up_to_now:\n",
    "                    blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        # Determine node colors\n",
    "        node_colors = []\n",
    "        for node in graph.nodes:\n",
    "            if node in processed_up_to_now:  # Already expanded nodes\n",
    "                color = 'black'\n",
    "            elif node == highlighted_node:  # Node being split\n",
    "                color = 'red'\n",
    "            elif node in blue_nodes_up_to_now:  # Nodes below the split node and not yet expanded\n",
    "                color = 'blue'\n",
    "            else:  # Unprocessed nodes\n",
    "                color = 'gray'\n",
    "            node_colors.append(color)\n",
    "\n",
    "        # Draw the graph with highlighting\n",
    "        nx.draw(\n",
    "            graph,\n",
    "            pos,\n",
    "            with_labels=True,\n",
    "            node_color=node_colors,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create the slider\n",
    "    interact(update, iteration=IntSlider(min=0, max=len(iterations) - 1, step=1, value=0))\n",
    "\n",
    "# Usage\n",
    "visualize_reachability_expansion_with_yifan_hu(full_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\wise-dag-ml\\lib\\site-packages\\pygraphviz\\agraph.py:1403: RuntimeWarning: Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2d2760411e459da213c42698198763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=74), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_reachability_expansion_with_yifan_hu(graph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Nodes not highlighted\n",
    "    (in red or blue) will be hidden at each iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    \"\"\"\n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append((list(active_nodes), None))\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true -Grepulsiveforce=5')\n",
    "\n",
    "    # Slider callback function\n",
    "    def update(iteration):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        nodes_to_draw, highlighted_node = iterations[iteration]\n",
    "        \n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration) if iterations[i][1] is not None\n",
    "        )\n",
    "        \n",
    "        blue_nodes_up_to_now = set(root_nodes)\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            if parent_node is not None:\n",
    "                for child in graph.successors(parent_node):\n",
    "                    if child not in processed_up_to_now:\n",
    "                        blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        visible_nodes = blue_nodes_up_to_now | {highlighted_node} if highlighted_node else blue_nodes_up_to_now\n",
    "        visible_nodes -= processed_up_to_now\n",
    "\n",
    "        subgraph = graph.subgraph(visible_nodes)\n",
    "        node_colors = [\n",
    "            'red' if node == highlighted_node else 'lightblue' for node in subgraph.nodes\n",
    "        ] if highlighted_node else ['lightblue' for node in subgraph.nodes]\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            node_color=node_colors,\n",
    "            node_size=200,\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            font_size=6,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # Create the slider\n",
    "    interact(update, iteration=IntSlider(min=0, max=len(iterations)-1, step=1, value=0))\n",
    "\n",
    "# Usage\n",
    "visualize_reachability_expansion_with_yifan_hu(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def visualize_reachability_expansion_with_yifan_hu_images(graph, output_folder=\"images\"):\n",
    "    \"\"\"\n",
    "    Create and save visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Saves each iteration\n",
    "    as an image in the specified output folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    - output_folder: Directory to save the images.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append((list(active_nodes), None))\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true -Grepulsiveforce=5')\n",
    "\n",
    "    # Save images for each iteration\n",
    "    for iteration, (nodes_to_draw, highlighted_node) in enumerate(iterations):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration) if iterations[i][1] is not None\n",
    "        )\n",
    "        \n",
    "        blue_nodes_up_to_now = set(root_nodes)\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            if parent_node is not None:\n",
    "                for child in graph.successors(parent_node):\n",
    "                    if child not in processed_up_to_now:\n",
    "                        blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        visible_nodes = blue_nodes_up_to_now | {highlighted_node} if highlighted_node else blue_nodes_up_to_now\n",
    "        visible_nodes -= processed_up_to_now\n",
    "\n",
    "        subgraph = graph.subgraph(visible_nodes)\n",
    "        node_colors = [\n",
    "            'red' if node == highlighted_node else 'lightblue' for node in subgraph.nodes\n",
    "        ] if highlighted_node else ['lightblue' for node in subgraph.nodes]\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            node_color=node_colors,\n",
    "            node_size=200,\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            font_size=6,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        file_path = os.path.join(output_folder, f\"iteration_{iteration}.png\")\n",
    "        plt.savefig(file_path, format='png', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reachability_expansion_with_yifan_hu_images(full_graph, output_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachability_iterations(graph, output_folder=\"images\"):\n",
    "    \"\"\"\n",
    "    Create and save visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Saves each iteration\n",
    "    as an image in the specified output folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    - output_folder: Directory to save the images.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append(list(active_nodes))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append(list(active_nodes))\n",
    "\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = reachability_iterations(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_dict = {index: sublist for index, sublist in enumerate(iterations)}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../data/iterations_dict.json','w') as file:\n",
    "    json.dump(iterations_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average DAG arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_df = pd.read_csv('../data/DAGs_standardized.csv', dtype={'Exposure': str, 'Outcome':str})\n",
    "dag_df['Exposure_standard_name'] = [code_to_name[i] for i in dag_df.Exposure]\n",
    "dag_df['Outcome_standard_name'] = [code_to_name[i] for i in dag_df.Outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Exposure_standard_name</th>\n",
       "      <th>Corrected_Direction</th>\n",
       "      <th>Outcome_standard_name</th>\n",
       "      <th>Exposure_name</th>\n",
       "      <th>Outcome_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise</td>\n",
       "      <td>first-ever ischemic stroke occurence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise</td>\n",
       "      <td>Stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise</td>\n",
       "      <td>Stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise (cardiac not excessive)</td>\n",
       "      <td>FIRST ISCHEMIC STROKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>physical exercise</td>\n",
       "      <td>second stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>physical exercise</td>\n",
       "      <td>first stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise</td>\n",
       "      <td>first ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>moderate physical activity</td>\n",
       "      <td>second ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>moderate physical activity</td>\n",
       "      <td>first ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>exercise</td>\n",
       "      <td>second ischemic stroke occurence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>physical activity</td>\n",
       "      <td>second ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>Exercises regularly</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>physical activity</td>\n",
       "      <td>first ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Lesion size</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Assessment using Modified Rankin Scale</td>\n",
       "      <td>lesion_size</td>\n",
       "      <td>mRS_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Lesion size</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Assessment using Modified Rankin Scale</td>\n",
       "      <td>lesion_size</td>\n",
       "      <td>mRS_follow_up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>Lesion size</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Assessment using Modified Rankin Scale</td>\n",
       "      <td>lesion size</td>\n",
       "      <td>functional post stroke outcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>Interleukin-6</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Postprocedural recovery status</td>\n",
       "      <td>systemic IL6</td>\n",
       "      <td>recovery level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>Interleukin-6</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Postprocedural recovery status</td>\n",
       "      <td>brain IL6_time2</td>\n",
       "      <td>recovery level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>Interleukin-6</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Postprocedural recovery status</td>\n",
       "      <td>brain IL6_time1</td>\n",
       "      <td>recovery level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>Able to mobilize</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>higher fitness/mobility pre</td>\n",
       "      <td>FIRST ISCHEMIC STROKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>Able to mobilize</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>higher fitness/mobility post</td>\n",
       "      <td>second ischemic stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Assessment using Modified Rankin Scale</td>\n",
       "      <td>second ischemic stroke</td>\n",
       "      <td>functional outcome (e.g. measured by mRS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Assessment using Modified Rankin Scale</td>\n",
       "      <td>1. stroke incidence</td>\n",
       "      <td>functional post stroke outcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>Age factor</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Smoker</td>\n",
       "      <td>age</td>\n",
       "      <td>smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>Age factor</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Smoker</td>\n",
       "      <td>age</td>\n",
       "      <td>Smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>Physical fitness state</td>\n",
       "      <td>Decrease</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>higher fitness/mobility pre</td>\n",
       "      <td>FIRST ISCHEMIC STROKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>Physical fitness state</td>\n",
       "      <td>Increase</td>\n",
       "      <td>Ischemic stroke</td>\n",
       "      <td>higher fitness/mobility post</td>\n",
       "      <td>second ischemic stroke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Exposure_standard_name Corrected_Direction  \\\n",
       "0    7     Exercises regularly            Decrease   \n",
       "1    5     Exercises regularly            Decrease   \n",
       "2    5     Exercises regularly            Decrease   \n",
       "3    6     Exercises regularly            Decrease   \n",
       "4   19     Exercises regularly            Decrease   \n",
       "5   19     Exercises regularly            Decrease   \n",
       "6   11     Exercises regularly            Decrease   \n",
       "7   10     Exercises regularly            Decrease   \n",
       "8   10     Exercises regularly            Decrease   \n",
       "9    7     Exercises regularly            Decrease   \n",
       "10   9     Exercises regularly            Increase   \n",
       "11   9     Exercises regularly            Increase   \n",
       "12   2             Lesion size            Increase   \n",
       "13   2             Lesion size            Increase   \n",
       "14   4             Lesion size            Decrease   \n",
       "15  20           Interleukin-6            Decrease   \n",
       "16  20           Interleukin-6            Decrease   \n",
       "17  20           Interleukin-6            Increase   \n",
       "18   6        Able to mobilize            Decrease   \n",
       "19   6        Able to mobilize            Increase   \n",
       "20  10         Ischemic stroke            Increase   \n",
       "21   4         Ischemic stroke            Decrease   \n",
       "22   9              Age factor            Decrease   \n",
       "23   1              Age factor            Increase   \n",
       "24   6  Physical fitness state            Decrease   \n",
       "25   6  Physical fitness state            Increase   \n",
       "\n",
       "                     Outcome_standard_name                     Exposure_name  \\\n",
       "0                          Ischemic stroke                          exercise   \n",
       "1                          Ischemic stroke                          exercise   \n",
       "2                          Ischemic stroke                          exercise   \n",
       "3                          Ischemic stroke  exercise (cardiac not excessive)   \n",
       "4                          Ischemic stroke                 physical exercise   \n",
       "5                          Ischemic stroke                 physical exercise   \n",
       "6                          Ischemic stroke                          exercise   \n",
       "7                          Ischemic stroke        moderate physical activity   \n",
       "8                          Ischemic stroke        moderate physical activity   \n",
       "9                          Ischemic stroke                          exercise   \n",
       "10                         Ischemic stroke                 physical activity   \n",
       "11                         Ischemic stroke                 physical activity   \n",
       "12  Assessment using Modified Rankin Scale                       lesion_size   \n",
       "13  Assessment using Modified Rankin Scale                       lesion_size   \n",
       "14  Assessment using Modified Rankin Scale                       lesion size   \n",
       "15          Postprocedural recovery status                      systemic IL6   \n",
       "16          Postprocedural recovery status                   brain IL6_time2   \n",
       "17          Postprocedural recovery status                   brain IL6_time1   \n",
       "18                         Ischemic stroke       higher fitness/mobility pre   \n",
       "19                         Ischemic stroke      higher fitness/mobility post   \n",
       "20  Assessment using Modified Rankin Scale            second ischemic stroke   \n",
       "21  Assessment using Modified Rankin Scale               1. stroke incidence   \n",
       "22                                  Smoker                               age   \n",
       "23                                  Smoker                               age   \n",
       "24                         Ischemic stroke       higher fitness/mobility pre   \n",
       "25                         Ischemic stroke      higher fitness/mobility post   \n",
       "\n",
       "                                 Outcome_name  \n",
       "0        first-ever ischemic stroke occurence  \n",
       "1                                      Stroke  \n",
       "2                                      Stroke  \n",
       "3                       FIRST ISCHEMIC STROKE  \n",
       "4                               second stroke  \n",
       "5                                first stroke  \n",
       "6                       first ischemic stroke  \n",
       "7                      second ischemic stroke  \n",
       "8                       first ischemic stroke  \n",
       "9            second ischemic stroke occurence  \n",
       "10                     second ischemic stroke  \n",
       "11                      first ischemic stroke  \n",
       "12                                 mRS_stroke  \n",
       "13                              mRS_follow_up  \n",
       "14             functional post stroke outcome  \n",
       "15                             recovery level  \n",
       "16                             recovery level  \n",
       "17                             recovery level  \n",
       "18                      FIRST ISCHEMIC STROKE  \n",
       "19                     second ischemic stroke  \n",
       "20  functional outcome (e.g. measured by mRS)  \n",
       "21             functional post stroke outcome  \n",
       "22                                    smoking  \n",
       "23                                    Smoking  \n",
       "24                      FIRST ISCHEMIC STROKE  \n",
       "25                     second ischemic stroke  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inconsistent_pairs = dag_df.groupby(['Exposure', 'Outcome']).filter(\n",
    "    lambda group: group['Corrected_Direction'].nunique() > 1\n",
    ")\n",
    "inconsistent_pairs.sort_values('Exposure')[['ID','Exposure_standard_name','Corrected_Direction','Outcome_standard_name','Exposure_name','Outcome_name',]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_iteration_columns_from_active_nodes(code_graph, df, iterations_data):\n",
    "#     \"\"\"\n",
    "#     Add columns to a DataFrame based on active nodes during each iteration.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - graph: A NetworkX DiGraph representing the graph.\n",
    "#     - df: A pandas DataFrame with at least the 'code' column.\n",
    "#     - iterations_data: A list of tuples (active_nodes, highlighted_node) \n",
    "#                        for each iteration, as generated by the visualization code.\n",
    "    \n",
    "#     Returns:\n",
    "#     - Updated DataFrame with new columns for each iteration.\n",
    "#     \"\"\"\n",
    "#     graph = replace_nodes_with_names(code_graph, code_to_name)\n",
    "\n",
    "#     # Prepare iteration columns\n",
    "#     iteration_columns = {f\"iteration_{i}\": [] for i in range(len(iterations_data))}\n",
    "    \n",
    "#     for iteration_idx, (active_nodes, _) in enumerate(iterations_data):\n",
    "#         active_nodes_set = set(active_nodes)\n",
    "        \n",
    "#         for name_term in df['standardized_term']:\n",
    "#             term = name_to_code[name_term]\n",
    "#             if term in active_nodes_set:\n",
    "#                 # If the term itself is active, it is its own parent\n",
    "#                 iteration_columns[f\"iteration_{iteration_idx}\"].append(term)\n",
    "#             else:\n",
    "#                 # Traverse upwards to find the closest active ancestor\n",
    "#                 closest_parent = term\n",
    "#                 for ancestor in nx.ancestors(graph, term):\n",
    "#                     if ancestor in active_nodes_set:\n",
    "#                         closest_parent = ancestor\n",
    "#                         break  # Stop at the first active ancestor\n",
    "                \n",
    "#                 iteration_columns[f\"iteration_{iteration_idx}\"].append(closest_parent)\n",
    "    \n",
    "#     # Add the iteration columns to the DataFrame\n",
    "#     for column_name, values in iteration_columns.items():\n",
    "#         df[column_name] = values\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# final_df = pd.DataFrame({'original_term':[code_to_original_name[i] for i in sorted(my_terms)],'code':sorted(my_terms), 'standardized_term':[code_to_name[i] for i in sorted(my_terms)]})\n",
    "\n",
    "# # Add iteration columns\n",
    "# updated_df = add_iteration_columns_from_active_nodes(full_graph, final_df, iterations)\n",
    "# updated_df.to_csv('terms_coded_and_with_hierarchy.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wise-dag-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
