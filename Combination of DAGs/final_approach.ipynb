{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx   \n",
    "from ipywidgets import interact, IntSlider\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from hierarchy_functions import get_unique_coded_terms, create_SNOMED_CT_graph_based_on_terms, find_lca_and_distance, compute_lcas_and_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DAG Hierarchy\n",
    "\n",
    "**Goal: Group terms and find common name to create a hierarchy of DAG nodes from broad down to specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data\n",
    "1. `dag_df` - Standardized DAG dataframe from workshop\n",
    "2. `concept_df` - Athena concept dataframe containing all the athena ids, concept codes, and concept names\n",
    "3. `concept_relationship_df` - Athena dataframe containing the relationships between all the concepts\n",
    "4. `concept_ancestor_df` - Athena dataframe containing information on the ancestors of terms (unfortunately seems incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_df = pd.read_csv('../data/DAGs_standardized.csv', dtype={'Exposure': str, 'Outcome':str})\n",
    "concept_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT.csv', sep='\\t', dtype={'concept_code': str, 'concept_id': str}, low_memory=False)\n",
    "concept_relationship_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT_RELATIONSHIP.csv', sep='\\t', dtype={'concept_id_1':str, 'concept_id_2': str}, low_memory=False)\n",
    "\n",
    "concept_df = concept_df[concept_df['vocabulary_id'] == 'SNOMED']\n",
    "concept_relationship_df = concept_relationship_df[\n",
    "    (concept_relationship_df['concept_id_1'].isin(concept_df['concept_id'])) &\n",
    "    (concept_relationship_df['concept_id_2'].isin(concept_df['concept_id']))]\n",
    "\n",
    "custom_concepts = pd.read_excel('../Standardization/custom_snomed_concepts_mapping.xlsx', sheet_name='concepts', dtype={'concept_id':str})\n",
    "custom_relationships = pd.read_excel('../Standardization/custom_snomed_concepts_mapping.xlsx', sheet_name='relationships', dtype={'concept_id_1':str, 'concept_id_2':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding concept class id to custom concepts based on their mapped parent relationship concept class id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_concepts = custom_concepts.merge(\n",
    "    custom_relationships.merge(concept_df, left_on='concept_id_1', right_on='concept_id')[['concept_id_2','concept_class_id']],\n",
    "    left_on='concept_id', right_on='concept_id_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add custom concepts and relationships to concept and relationships database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['concept_id','concept_name','concept_class_id']\n",
    "concept_df = pd.concat([concept_df[cols1], custom_concepts[cols1]])\n",
    "\n",
    "cols2 = ['concept_id_1', 'concept_id_2', 'relationship_id']\n",
    "concept_relationship_df = pd.concat([concept_relationship_df[cols2], custom_relationships[cols2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get set of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_terms = get_unique_coded_terms(dag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary to convert codes to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_name = dict(zip(concept_df[\"concept_id\"], concept_df[\"concept_name\"]))\n",
    "name_to_code = dict(zip(concept_df[\"concept_name\"], concept_df[\"concept_id\"]))\n",
    "\n",
    "my_terms_written = [code_to_name[code] for code in my_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to original name map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_original_name = pd.read_csv('..\\Standardization\\complete_athena_id_mapping.csv', dtype={'target_concept_id': str}).groupby('target_concept_id').agg(list).to_dict()['source_code_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_relationship_ids = [\"Subsumes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchy(my_terms_filtered, G_clinical, compute_lcas_and_distances, find_lca_and_distance):\n",
    "    \"\"\"\n",
    "    Build a hierarchy from terms using a graph, precomputing LCAs and distances.\n",
    "\n",
    "    Parameters:\n",
    "    - my_terms_filtered (set): A set of terms to process.\n",
    "    - G_clinical (networkx.Graph): The graph representing the relationships.\n",
    "    - compute_lcas_and_distances (function): A function that computes LCAs and distances for initial terms.\n",
    "    - find_lca_and_distance (function): A function to compute the LCA and distance for a pair of terms.\n",
    "\n",
    "    Returns:\n",
    "    - hierarchy (defaultdict): The hierarchy built from the terms.\n",
    "    \"\"\"\n",
    "    # Precompute LCAs and distances\n",
    "    lca_distances = compute_lcas_and_distances(my_terms_filtered, G_clinical)\n",
    "\n",
    "    remaining_terms = my_terms_filtered.copy()\n",
    "    hierarchy = defaultdict(list)\n",
    "    processed_pairs = set()  # Track processed pairs to avoid infinite loops\n",
    "\n",
    "    def has_descendants(term, remaining_terms, graph):\n",
    "        \"\"\"Check if a term has descendants in the remaining terms.\"\"\"\n",
    "        descendants = nx.descendants(graph, term)\n",
    "        return bool(descendants & set(remaining_terms))\n",
    "    \n",
    "    # Special case: If there is only one node, add it to the hierarchy and return\n",
    "    if len(remaining_terms) == 1:\n",
    "        single_node = next(iter(remaining_terms))  # Get the single node\n",
    "        hierarchy[single_node] = []  # No children\n",
    "        return hierarchy\n",
    "\n",
    "    while len(remaining_terms) > 1:\n",
    "        shortest_distance = float(\"inf\")\n",
    "        best_pair = None\n",
    "        best_lca = None\n",
    "\n",
    "        # Find the best pair of terms based on distance\n",
    "        for node1 in sorted(remaining_terms):\n",
    "            for node2 in sorted(remaining_terms):\n",
    "                if node1 >= node2: # skip duplicates or reverse pairs\n",
    "                    continue\n",
    "                key = frozenset({node1, node2})\n",
    "                if key in processed_pairs:\n",
    "                    continue\n",
    "                if key not in lca_distances[1]:\n",
    "                    lca, distance = find_lca_and_distance(G_clinical, node1, node2)\n",
    "                    lca_distances[0][key] = lca\n",
    "                    lca_distances[1][key] = distance\n",
    "                dist = lca_distances[1][key]\n",
    "\n",
    "                # Update the best pair if this distance is shorter\n",
    "                if dist < shortest_distance or (dist == shortest_distance and (node1, node2) < best_pair): # tie break to prevent random result\n",
    "                    shortest_distance = dist\n",
    "                    best_pair = (node1, node2)\n",
    "                    best_lca = lca_distances[0][key]\n",
    "\n",
    "        if best_pair is None:\n",
    "            break\n",
    "        \n",
    "        hierarchy[best_lca].append(best_pair)\n",
    "        processed_pairs.add(frozenset(best_pair))\n",
    "\n",
    "        # Replace grouped nodes with their LCA if they no longer have descendants\n",
    "        to_remove = set()\n",
    "        to_add = set()\n",
    "\n",
    "        for node in best_pair:\n",
    "            if not has_descendants(node, remaining_terms, G_clinical):\n",
    "                to_remove.add(node)\n",
    "        to_add.add(best_lca)\n",
    "\n",
    "        # Apply changes after the iteration\n",
    "        remaining_terms -= to_remove\n",
    "        remaining_terms |= to_add\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def hierarchy_to_graph(hierarchy, original_terms):\n",
    "    \"\"\"\n",
    "    Convert a hierarchy dictionary to a directed graph, removing duplicates and self-references.\n",
    "    Adds a 'type' attribute to distinguish between original and LCA nodes.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Iterate over the hierarchy\n",
    "    for lca, pairs in hierarchy.items():\n",
    "        # Add the LCA node and mark it as an 'lca'\n",
    "        if lca not in graph:\n",
    "            node_type = \"original\" if lca in original_terms else \"lca\"\n",
    "            graph.add_node(lca, type=node_type)\n",
    "\n",
    "        for pair in pairs:\n",
    "            for node in pair:\n",
    "                # Add the original terms or nodes if not already added\n",
    "                if node not in graph:\n",
    "                    node_type = \"original\" if node in original_terms else \"lca\"\n",
    "                    graph.add_node(node, type=node_type)\n",
    "\n",
    "                # Add edges, avoiding self-references\n",
    "                if lca != node:\n",
    "                    if not graph.has_edge(lca, node):\n",
    "                        graph.add_edge(lca, node)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def prune_redundant_edges(graph):\n",
    "    \"\"\"Remove redundant edges that create shortcuts in the hierarchy.\"\"\"\n",
    "    edges_to_remove = set()\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        for descendant in descendants:\n",
    "            for intermediate in graph.successors(node):\n",
    "                if intermediate in descendants and graph.has_edge(intermediate, descendant):\n",
    "                    edges_to_remove.add((node, descendant))\n",
    "\n",
    "    graph.remove_edges_from(edges_to_remove)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def replace_nodes_with_names(graph, mapping):\n",
    "    \"\"\"Replace node codes with names based on a mapping dictionary while preserving attributes.\"\"\"\n",
    "\n",
    "    new_graph = nx.DiGraph()\n",
    "\n",
    "    # Add nodes with attributes, replacing codes with names\n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        new_node = mapping.get(node, node)  # Replace code with name if mapping exists\n",
    "        new_graph.add_node(new_node, **attrs)  # Preserve attributes\n",
    "\n",
    "    # Add edges, replacing codes with names\n",
    "    for u, v, attrs in graph.edges(data=True):\n",
    "        new_u = mapping.get(u, u)\n",
    "        new_v = mapping.get(v, v)\n",
    "        new_graph.add_edge(new_u, new_v, **attrs)  # Preserve edge attributes\n",
    "\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Substance',\n",
       " 'Clinical Finding',\n",
       " 'Procedure',\n",
       " 'Observable Entity',\n",
       " 'Context-dependent',\n",
       " 'Morph Abnormality',\n",
       " 'Social Context',\n",
       " 'Event']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_classes = list(concept_df[concept_df.concept_id.isin(my_terms)].concept_class_id.unique())\n",
    "concept_classes.remove('Disorder')\n",
    "concept_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 12 terms to the graph for concept class Substance.\n",
      "\n",
      "Adding 166 terms to the graph for concept class ['Clinical Finding', 'Disorder'].\n",
      "\n",
      "Adding 44 terms to the graph for concept class Procedure.\n",
      "\n",
      "Adding 8 terms to the graph for concept class Observable Entity.\n",
      "\n",
      "Adding 2 terms to the graph for concept class Context-dependent.\n",
      "\n",
      "Adding 13 terms to the graph for concept class Morph Abnormality.\n",
      "\n",
      "Adding 4 terms to the graph for concept class Social Context.\n",
      "\n",
      "Adding 1 terms to the graph for concept class Event.\n",
      "\n",
      "Total number of terms in the Snomed DAG are 250 with 251 edges.\n"
     ]
    }
   ],
   "source": [
    "full_graph = nx.DiGraph()\n",
    "\n",
    "for concept_class in concept_classes:\n",
    "\n",
    "    # Group clinical finding and disorder together\n",
    "    if concept_class=='Clinical Finding':\n",
    "        concept_class = ['Clinical Finding', 'Disorder']\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&(concept_df.concept_class_id.isin(concept_class))].concept_id)\n",
    "\n",
    "    else:\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&(concept_df.concept_class_id==concept_class)].concept_id)\n",
    "\n",
    "    G_x = create_SNOMED_CT_graph_based_on_terms(filtered_terms, concept_relationship_df, selected_relationship_ids)\n",
    "    \n",
    "    hierarchy_x = build_hierarchy(filtered_terms, G_x, compute_lcas_and_distances, find_lca_and_distance)\n",
    "    graph_x = hierarchy_to_graph(hierarchy_x, filtered_terms)\n",
    "    pruned_graph_x = prune_redundant_edges(graph_x)\n",
    "    graph_with_names_x = replace_nodes_with_names(pruned_graph_x, code_to_name)\n",
    "    \n",
    "    full_graph = nx.compose(full_graph, graph_with_names_x)\n",
    "    print(f'Adding {graph_with_names_x.number_of_nodes()} terms to the graph for concept class {concept_class}.\\n')\n",
    "print(f'Total number of terms in the Snomed DAG are {full_graph.number_of_nodes()} with {full_graph.number_of_edges()} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(full_graph, 'snomed_alternative_grouping_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: ['Substance', 'Clinical finding', 'Procedure', 'Observable entity', 'Morphologically abnormal structure', 'Social context', 'Death', 'FH: Cardiovascular disease']\n",
      "Iteration 1: ['Age factor', 'Finding related to ability to mobilize', 'Disease', 'Procedure with a clinical finding focus', 'Finding of lesion', 'Able to cope', 'Regimes and therapies', 'General Public Awareness of FAST-Scheme', 'Immune system finding', 'Uses oral contraception', 'Complete obstruction', 'Weight loss', 'Antidiabetic agent', 'Prediabetes', 'Biological substance', 'Assessment using assessment scale', 'Blood glucose management', 'Pregnancy', 'Lesion size', 'Admission to stroke unit', 'Quality of life satisfaction', 'Disability', 'Education and/or schooling finding', 'Procedure involving urinary catheter', 'Vascular sclerosis', 'Social isolation', 'Activities of daily living assessment', 'Preventive procedure', 'Male', 'Falls', 'Functionally dependent', 'Economic status', 'Removal', 'Body temperature above reference range', 'Time to therapy', 'Stress', 'Finding by site', 'Dietary finding', 'Evaluation finding', 'Introduction procedure', 'Physical fitness state', 'Time until diagnosis', 'N-terminal pro-B-type natriuretic peptide', 'Glomerular filtration rate', 'Racial group', 'Stenosis', 'Patient position finding', 'Production of reactive oxygen species', 'Urinary incontinence', 'Impaired cognition', 'Risk of cardiovascular disease', 'Neurological finding', 'Growth alteration', 'Social worker involved', 'Finding of renal function', 'Health-related behavior finding', 'Blood-brain barrier', 'Postprocedural recovery status', 'Family history of stroke', 'Bleeding', 'Surgical procedure', 'Increased body mass index', 'Decreased vascular flow']\n",
      "Iteration 2: ['Injection of botulinum toxin', 'Metabolic syndrome X', 'Pain', 'Assessment using National Institutes of Health stroke scale', 'Dyslipidemia', 'Glucagon-like peptide 1 receptor agonist', 'Paralysis', 'Arteriosclerosis', 'Disease', 'Hypercoagulability state', 'Able to mobilize', 'Infectious disease', 'Interleukin-6', 'Activity exercise pattern', 'Sympathetic activation', 'Assessment using Modified Rankin Scale', 'Administration of medication', 'Aphasia', 'Kidney disease', 'Mediterranean diet', 'High carbohydrate diet', 'Low sodium diet', 'Complicated atheromatous plaque', 'Fibromuscular dysplasia', 'Central nervous system complication', 'Continuous positive airway pressure ventilation treatment', 'Spasticity', 'Smoking cessation therapy', 'Immunosuppression', 'Cardiovascular finding', 'Troponin', 'General finding of soft tissue', 'DNA damage', 'Depressive disorder', 'Disorder by body site', 'Pain management', 'Decreased alcohol consumption', 'Sodium glucose cotransporter subtype 2 inhibitor', 'Secondary prevention', 'Neuronal survival', 'Sequelae of neurological disorders', 'Congenital malformation', 'Speech therapy', 'Silent micro-hemorrhage of brain', 'Leukocyte-derived microvesicles', 'Aspiration', 'Finding of brain', 'High fat diet', 'Organ or tissue vascular perfusion decreased', 'Leuko-araiosis', 'Healthy diet', 'Current drinker', 'Measurement finding', 'Proliferation', 'Smoker', 'Malignant neoplastic disease', 'Passive smoker', 'Thiazolidinedione', 'Psychotherapy', 'Peripheral immune competence', 'B-cell infiltration', 'Neurological finding', 'Endothelial microvesicles', 'Vascular surgery procedure', 'Sleep disorder', 'Inflammatory disorder', 'Nutritional disorder', 'Genetic predisposition', 'Impaired mobility', 'Cognitive skills training', 'Obesity', 'Primary prevention', 'Disorder of glucose metabolism', 'Removal of thrombus', 'Dysfunction of urinary bladder', 'Care regime']\n",
      "Iteration 3: ['Finding of substance level', 'Heavy drinker', 'Physical therapy procedure', 'Sleep apnea', 'Insertion of carotid artery stent', 'Gets no exercise', 'Seizure', 'Moderate drinker', 'Measurement finding above reference range', 'Percutaneous transluminal procedure on blood vessel', 'Cardiac troponin T', 'Loss of brain structural integrity', 'COVID-19', 'Atherosclerosis', 'Disorder of soft tissue', 'Exercises regularly', 'Chronic kidney disease', 'Cardiovascular finding', 'General finding of soft tissue', 'Cachexia', 'Cardiac finding', 'Endarterectomy', 'Disorder of body system', 'Disorder by body site', 'Cardiac syndrome X', 'Urinary tract infectious disease', 'Cerebral atrophy', 'Chemotherapy', 'Enlarged perivascular spaces', 'Endothelial proliferation', 'Microglial proliferation', 'Increased cardiac stroke volume', 'Angioplasty of carotid artery', 'Encephalitis', 'Epilepsy due to cerebrovascular accident', 'Pulmonary embolism', 'Delirium', 'Dyssomnia', 'Drug therapy', 'Lesion of brain', 'Thrombolysis of artery', 'Inflammation of specific body structures or tissue', 'Ischemic stroke', 'Increased collateral circulation', 'Disorder of glucose metabolism', 'Astrocytic proliferation', 'Systemic inflammatory response syndrome']\n",
      "Iteration 4: ['Soft tissue lesion', 'Disorder of endocrine system', 'Sickling disorder due to hemoglobin S', 'Disorder of autonomic nervous system', 'Antidepressant therapy', 'Eclampsia', 'Disorder of soft tissue', 'Finding of blood substance level', 'Cardiac finding', 'Administration of prophylactic statin', 'Pneumonia', 'Platelet aggregation inhibitor therapy', 'Fracture of bone', 'Cardiac remodeling', 'Catecholamine level - finding', 'Lipoprotein (a) hyperlipoproteinemia', 'C-reactive protein above reference range', 'Carotid endarterectomy', 'Dysphagia', 'Cerebral infarction', 'Anticoagulant therapy', 'Diabetes mellitus', 'Hyperlipidemia', 'Antihypertensive therapy', 'Disorder of cardiovascular system', 'Finding of oxygen saturation', 'Antibiotic therapy']\n",
      "Iteration 5: ['Soft tissue lesion', 'Transient cerebral ischemia', 'Hypercholesterolemia', 'Insulin resistance', 'Moyamoya disease', 'Low blood pressure', 'Hypertensive disorder', 'Female sex hormones - serum level - finding', 'Lacunar infarction', 'Cerebral venous sinus thrombosis', 'Gestational diabetes mellitus complicating pregnancy', 'Heart disease', 'Thrombosis', 'Blood glucose level - finding', 'Dissection of artery', 'Vascular disorder', 'Dissection of cerebral artery']\n",
      "Iteration 6: ['Hypertension complicating pregnancy', 'Myocardial infarction', 'Deep venous thrombosis', 'Embolism', 'Pre-eclampsia', 'Patent foramen ovale', 'Vasculitis', 'Acute infective endocarditis', 'Cardiac arrhythmia', 'Takotsubo cardiomyopathy', 'Migraine', 'Thrombus of cardiac chamber', 'Endocarditis', 'Disorder of cardiac function']\n",
      "Iteration 7: ['Myocardial infarction due to atherothrombotic coronary artery disease', 'Myocardial infarction due to demand ischemia', 'Heart failure', 'Migraine with aura', 'Atrial fibrillation']\n",
      "Iteration 8: ['Acute heart failure', 'Chronic heart failure']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def synchronized_expansion_with_reappearance(full_graph):\n",
    "    \"\"\"\n",
    "    Perform synchronized hierarchical expansion on a DAG,\n",
    "    keeping nodes active and visible in every iteration until all their children are expanded.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_graph: A NetworkX DiGraph object representing the DAG.\n",
    "    \n",
    "    Returns:\n",
    "    - iterations: A list of lists, where each inner list contains the nodes expanded at that iteration.\n",
    "    \"\"\"\n",
    "    # Step 1: Track parent dependencies (number of parents yet to be processed)\n",
    "    parent_dependencies = {node: full_graph.in_degree(node) for node in full_graph.nodes}\n",
    "\n",
    "    # Step 2: Identify root nodes (those with no incoming edges)\n",
    "    roots = [node for node, indegree in parent_dependencies.items() if indegree == 0]\n",
    "\n",
    "    # Step 3: Iterative expansion with multiple parent handling\n",
    "    processed = set()  # Set to track processed nodes\n",
    "    iterations = []  # List to store nodes expanded at each iteration\n",
    "    current_level = set(roots)  # Start with root nodes\n",
    "\n",
    "    while current_level:\n",
    "        iterations.append(list(current_level))  # Add the current level to the result\n",
    "        next_level = set(current_level)  # Include current nodes by default\n",
    "\n",
    "        for node in current_level:\n",
    "            # Add children to next level if all their parents are processed\n",
    "            for child in full_graph.successors(node):\n",
    "                if child not in processed:\n",
    "                    # Decrement the dependency counter for each child\n",
    "                    parent_dependencies[child] -= 1\n",
    "                    # If all parents are processed, the child becomes eligible\n",
    "                    if parent_dependencies[child] == 0:\n",
    "                        next_level.add(child)\n",
    "\n",
    "            # Only mark the node as processed if all its children are processed\n",
    "            if all(\n",
    "                child in processed or child in next_level\n",
    "                for child in full_graph.successors(node)\n",
    "            ):\n",
    "                processed.add(node)  # Mark the node as fully processed\n",
    "\n",
    "        # Filter out fully processed nodes from the current level\n",
    "        current_level = {node for node in next_level if node not in processed}\n",
    "\n",
    "    return iterations\n",
    "\n",
    "# Example usage\n",
    "iterations = synchronized_expansion_with_reappearance(full_graph)\n",
    "for i, level in enumerate(iterations):\n",
    "    print(f\"Iteration {i}: {level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_depths(graph):\n",
    "    \"\"\"\n",
    "    Precompute the depths (max depth to leaf) for all nodes in the graph.\n",
    "    \"\"\"\n",
    "    depths = {}\n",
    "\n",
    "    def compute_depth(node):\n",
    "        if node in depths:\n",
    "            return depths[node]\n",
    "        if graph.out_degree(node) == 0:  # Leaf node\n",
    "            depths[node] = 0\n",
    "        else:\n",
    "            depths[node] = 1 + max(compute_depth(child) for child in graph.successors(node))\n",
    "        return depths[node]\n",
    "    \n",
    "    for node in nx.topological_sort(graph):\n",
    "        compute_depth(node)\n",
    "    \n",
    "    return depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\wise-dag-ml\\lib\\site-packages\\pygraphviz\\agraph.py:1403: RuntimeWarning: Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ffcdb78abb40a28acd8c33ecb7cd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=73), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_reachability_expansion_with_yifan_hu(graph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    \"\"\"\n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true')\n",
    "\n",
    "    # Slider callback function\n",
    "    def update(iteration):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        nodes_to_draw, highlighted_node = iterations[iteration]\n",
    "        \n",
    "        # Collect processed nodes up to the current iteration\n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration)\n",
    "        )\n",
    "        \n",
    "        # Collect blue nodes up to the current iteration\n",
    "        blue_nodes_up_to_now = set(root_nodes)  # Start with root nodes\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            for child in graph.successors(parent_node):\n",
    "                if child not in processed_up_to_now:\n",
    "                    blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        # Determine node colors\n",
    "        node_colors = []\n",
    "        for node in graph.nodes:\n",
    "            if node in processed_up_to_now:  # Already expanded nodes\n",
    "                color = 'black'\n",
    "            elif node == highlighted_node:  # Node being split\n",
    "                color = 'red'\n",
    "            elif node in blue_nodes_up_to_now:  # Nodes below the split node and not yet expanded\n",
    "                color = 'blue'\n",
    "            else:  # Unprocessed nodes\n",
    "                color = 'gray'\n",
    "            node_colors.append(color)\n",
    "\n",
    "        # Draw the graph with highlighting\n",
    "        nx.draw(\n",
    "            graph,\n",
    "            pos,\n",
    "            with_labels=True,\n",
    "            node_color=node_colors,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create the slider\n",
    "    interact(update, iteration=IntSlider(min=0, max=len(iterations) - 1, step=1, value=0))\n",
    "\n",
    "# Usage\n",
    "visualize_reachability_expansion_with_yifan_hu(full_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\wise-dag-ml\\lib\\site-packages\\pygraphviz\\agraph.py:1403: RuntimeWarning: Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112bb8d9bf5b46b2974a4f2fec78046d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=74), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_reachability_expansion_with_yifan_hu(graph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Nodes not highlighted\n",
    "    (in red or blue) will be hidden at each iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    \"\"\"\n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append((list(active_nodes), None))\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true -Grepulsiveforce=5')\n",
    "\n",
    "    # Slider callback function\n",
    "    def update(iteration):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        nodes_to_draw, highlighted_node = iterations[iteration]\n",
    "        \n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration) if iterations[i][1] is not None\n",
    "        )\n",
    "        \n",
    "        blue_nodes_up_to_now = set(root_nodes)\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            if parent_node is not None:\n",
    "                for child in graph.successors(parent_node):\n",
    "                    if child not in processed_up_to_now:\n",
    "                        blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        visible_nodes = blue_nodes_up_to_now | {highlighted_node} if highlighted_node else blue_nodes_up_to_now\n",
    "        visible_nodes -= processed_up_to_now\n",
    "\n",
    "        subgraph = graph.subgraph(visible_nodes)\n",
    "        node_colors = [\n",
    "            'red' if node == highlighted_node else 'lightblue' for node in subgraph.nodes\n",
    "        ] if highlighted_node else ['lightblue' for node in subgraph.nodes]\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            node_color=node_colors,\n",
    "            node_size=200,\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            font_size=6,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    # Create the slider\n",
    "    interact(update, iteration=IntSlider(min=0, max=len(iterations)-1, step=1, value=0))\n",
    "\n",
    "# Usage\n",
    "visualize_reachability_expansion_with_yifan_hu(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def visualize_reachability_expansion_with_yifan_hu_images(graph, output_folder=\"images\"):\n",
    "    \"\"\"\n",
    "    Create and save visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Saves each iteration\n",
    "    as an image in the specified output folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    - output_folder: Directory to save the images.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append((list(active_nodes), None))\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true -Grepulsiveforce=5')\n",
    "\n",
    "    # Save images for each iteration\n",
    "    for iteration, (nodes_to_draw, highlighted_node) in enumerate(iterations):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration) if iterations[i][1] is not None\n",
    "        )\n",
    "        \n",
    "        blue_nodes_up_to_now = set(root_nodes)\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            if parent_node is not None:\n",
    "                for child in graph.successors(parent_node):\n",
    "                    if child not in processed_up_to_now:\n",
    "                        blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        visible_nodes = blue_nodes_up_to_now | {highlighted_node} if highlighted_node else blue_nodes_up_to_now\n",
    "        visible_nodes -= processed_up_to_now\n",
    "\n",
    "        subgraph = graph.subgraph(visible_nodes)\n",
    "        node_colors = [\n",
    "            'red' if node == highlighted_node else 'lightblue' for node in subgraph.nodes\n",
    "        ] if highlighted_node else ['lightblue' for node in subgraph.nodes]\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            node_color=node_colors,\n",
    "            node_size=200,\n",
    "        )\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph,\n",
    "            pos,\n",
    "            font_size=6,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        file_path = os.path.join(output_folder, f\"iteration_{iteration}.png\")\n",
    "        plt.savefig(file_path, format='png', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reachability_expansion_with_yifan_hu_images(full_graph, output_folder=\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachability_iterations(graph, output_folder=\"images\"):\n",
    "    \"\"\"\n",
    "    Create and save visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning. Saves each iteration\n",
    "    as an image in the specified output folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    - output_folder: Directory to save the images.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        least_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), least_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == least_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(least_specific_node)\n",
    "        blue_nodes.discard(least_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "    \n",
    "    if active_nodes:\n",
    "        iterations.append((list(active_nodes), None))\n",
    "\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = reachability_iterations(full_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_iteration_columns_from_active_nodes(graph, df, iterations_data):\n",
    "    \"\"\"\n",
    "    Add columns to a DataFrame based on active nodes during each iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph representing the graph.\n",
    "    - df: A pandas DataFrame with at least the 'code' column.\n",
    "    - iterations_data: A list of tuples (active_nodes, highlighted_node) \n",
    "                       for each iteration, as generated by the visualization code.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated DataFrame with new columns for each iteration.\n",
    "    \"\"\"\n",
    "    # Prepare iteration columns\n",
    "    iteration_columns = {f\"iteration_{i}\": [] for i in range(len(iterations_data))}\n",
    "    \n",
    "    for iteration_idx, (active_nodes, _) in enumerate(iterations_data):\n",
    "        active_nodes_set = set(active_nodes)\n",
    "        \n",
    "        for term in df['standardized_term']:\n",
    "            if term in active_nodes_set:\n",
    "                # If the term itself is active, it is its own parent\n",
    "                iteration_columns[f\"iteration_{iteration_idx}\"].append(term)\n",
    "            else:\n",
    "                # Traverse upwards to find the closest active ancestor\n",
    "                closest_parent = term\n",
    "                for ancestor in nx.ancestors(graph, term):\n",
    "                    if ancestor in active_nodes_set:\n",
    "                        closest_parent = ancestor\n",
    "                        break  # Stop at the first active ancestor\n",
    "                \n",
    "                iteration_columns[f\"iteration_{iteration_idx}\"].append(closest_parent)\n",
    "    \n",
    "    # Add the iteration columns to the DataFrame\n",
    "    for column_name, values in iteration_columns.items():\n",
    "        df[column_name] = values\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "final_df = pd.DataFrame({'original_term':[code_to_original_name[i] for i in sorted(my_terms)],'code':sorted(my_terms), 'standardized_term':[code_to_name[i] for i in sorted(my_terms)]})\n",
    "\n",
    "# Add iteration columns\n",
    "updated_df = add_iteration_columns_from_active_nodes(full_graph, final_df, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('terms_coded_and_with_hierarchy.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wise-dag-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
