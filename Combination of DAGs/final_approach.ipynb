{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from hierarchy_functions import get_unique_coded_terms, create_SNOMED_CT_graph_based_on_terms, find_lca_and_distance, compute_lcas_and_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DAG Hierarchy\n",
    "\n",
    "**Goal: Group terms and find common name to create a hierarchy of DAG nodes from broad down to specific.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data\n",
    "1. `dag_df` - Standardized DAG dataframe from workshop\n",
    "2. `concept_df` - Athena concept dataframe containing all the athena ids, concept codes, and concept names\n",
    "3. `concept_relationship_df` - Athena dataframe containing the relationships between all the concepts\n",
    "4. `concept_ancestor_df` - Athena dataframe containing information on the ancestors of terms (unfortunately seems incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_df = pd.read_csv('../data/DAGs_standardized.csv', dtype={'Exposure': str, 'Outcome':str})\n",
    "concept_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT.csv', sep='\\t', dtype={'concept_code': str, 'concept_id': str}, low_memory=False)\n",
    "concept_relationship_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT_RELATIONSHIP.csv', sep='\\t', dtype={'concept_id_1':str, 'concept_id_2': str}, low_memory=False)\n",
    "concept_ancestor_df = pd.read_csv('../Standardization/athena_vocabulary/CONCEPT_ANCESTOR.csv', sep='\\t',dtype={'ancestor_concept_id': str, 'descendant_concept_id': str}, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get set of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_terms = get_unique_coded_terms(dag_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary to convert codes to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_name = dict(zip(concept_df[\"concept_id\"], concept_df[\"concept_name\"]))\n",
    "name_to_code = dict(zip(concept_df[\"concept_name\"], concept_df[\"concept_id\"]))\n",
    "my_terms_written = [code_to_name[code] for code in my_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_relationship_ids = [\"Subsumes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchy(my_terms_filtered, G_clinical, compute_lcas_and_distances, find_lca_and_distance):\n",
    "    \"\"\"\n",
    "    Build a hierarchy from terms using a graph, precomputing LCAs and distances.\n",
    "\n",
    "    Parameters:\n",
    "    - my_terms_filtered (set): A set of terms to process.\n",
    "    - G_clinical (networkx.Graph): The graph representing the relationships.\n",
    "    - compute_lcas_and_distances (function): A function that computes LCAs and distances for initial terms.\n",
    "    - find_lca_and_distance (function): A function to compute the LCA and distance for a pair of terms.\n",
    "\n",
    "    Returns:\n",
    "    - hierarchy (defaultdict): The hierarchy built from the terms.\n",
    "    \"\"\"\n",
    "    # Precompute LCAs and distances\n",
    "    lca_distances = compute_lcas_and_distances(my_terms_filtered, G_clinical)\n",
    "\n",
    "    remaining_terms = my_terms_filtered.copy()\n",
    "    hierarchy = defaultdict(list)\n",
    "    processed_pairs = set()  # Track processed pairs to avoid infinite loops\n",
    "\n",
    "    def has_descendants(term, remaining_terms, graph):\n",
    "        \"\"\"Check if a term has descendants in the remaining terms.\"\"\"\n",
    "        descendants = nx.descendants(graph, term)\n",
    "        return bool(descendants & set(remaining_terms))\n",
    "    \n",
    "    # Special case: If there is only one node, add it to the hierarchy and return\n",
    "    if len(remaining_terms) == 1:\n",
    "        single_node = next(iter(remaining_terms))  # Get the single node\n",
    "        hierarchy[single_node] = []  # No children\n",
    "        return hierarchy\n",
    "\n",
    "    while len(remaining_terms) > 1:\n",
    "        shortest_distance = float(\"inf\")\n",
    "        best_pair = None\n",
    "        best_lca = None\n",
    "\n",
    "        # Find the best pair of terms based on distance\n",
    "        for node1 in remaining_terms:\n",
    "            for node2 in remaining_terms:\n",
    "                if node1 == node2:\n",
    "                    continue\n",
    "                key = frozenset({node1, node2})\n",
    "                if key in processed_pairs:\n",
    "                    continue\n",
    "                if key not in lca_distances[1]:\n",
    "                    lca, distance = find_lca_and_distance(G_clinical, node1, node2)\n",
    "                    lca_distances[0][key] = lca\n",
    "                    lca_distances[1][key] = distance\n",
    "                dist = lca_distances[1][key]\n",
    "\n",
    "                # Update the best pair if this distance is shorter\n",
    "                if dist < shortest_distance:\n",
    "                    shortest_distance = dist\n",
    "                    best_pair = (node1, node2)\n",
    "                    best_lca = lca_distances[0][key]\n",
    "\n",
    "        if best_pair is None:\n",
    "            break\n",
    "        \n",
    "        hierarchy[best_lca].append(best_pair)\n",
    "        processed_pairs.add(frozenset(best_pair))\n",
    "\n",
    "        # Replace grouped nodes with their LCA if they no longer have descendants\n",
    "        for node in best_pair:\n",
    "            if not has_descendants(node, remaining_terms, G_clinical):\n",
    "                remaining_terms.remove(node)\n",
    "        remaining_terms.add(best_lca)\n",
    "\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_to_graph(hierarchy, original_terms):\n",
    "    \"\"\"\n",
    "    Convert a hierarchy dictionary to a directed graph, removing duplicates and self-references.\n",
    "    Adds a 'type' attribute to distinguish between original and LCA nodes.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Iterate over the hierarchy\n",
    "    for lca, pairs in hierarchy.items():\n",
    "        # Add the LCA node and mark it as an 'lca'\n",
    "        if lca not in graph:\n",
    "            node_type = \"original\" if lca in original_terms else \"lca\"\n",
    "            graph.add_node(lca, type=node_type)\n",
    "\n",
    "        for pair in pairs:\n",
    "            for node in pair:\n",
    "                # Add the original terms or nodes if not already added\n",
    "                if node not in graph:\n",
    "                    node_type = \"original\" if node in original_terms else \"lca\"\n",
    "                    graph.add_node(node, type=node_type)\n",
    "\n",
    "                # Add edges, avoiding self-references\n",
    "                if lca != node:\n",
    "                    if not graph.has_edge(lca, node):\n",
    "                        graph.add_edge(lca, node)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_redundant_edges(graph):\n",
    "    \"\"\"Remove redundant edges that create shortcuts in the hierarchy.\"\"\"\n",
    "    edges_to_remove = set()\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        for descendant in descendants:\n",
    "            for intermediate in graph.successors(node):\n",
    "                if intermediate in descendants and graph.has_edge(intermediate, descendant):\n",
    "                    edges_to_remove.add((node, descendant))\n",
    "\n",
    "    graph.remove_edges_from(edges_to_remove)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nodes_with_names(graph, mapping):\n",
    "    \"\"\"Replace node codes with names based on a mapping dictionary while preserving attributes.\"\"\"\n",
    "\n",
    "    new_graph = nx.DiGraph()\n",
    "\n",
    "    # Add nodes with attributes, replacing codes with names\n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        new_node = mapping.get(node, node)  # Replace code with name if mapping exists\n",
    "        new_graph.add_node(new_node, **attrs)  # Preserve attributes\n",
    "\n",
    "    # Add edges, replacing codes with names\n",
    "    for u, v, attrs in graph.edges(data=True):\n",
    "        new_u = mapping.get(u, u)\n",
    "        new_v = mapping.get(v, v)\n",
    "        new_graph.add_edge(new_u, new_v, **attrs)  # Preserve edge attributes\n",
    "\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Substance',\n",
       " 'Clinical Finding',\n",
       " 'Procedure',\n",
       " 'Observable Entity',\n",
       " 'Context-dependent',\n",
       " 'Morph Abnormality',\n",
       " 'Social Context',\n",
       " 'Event']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_classes = list(concept_df[concept_df.concept_id.isin(my_terms)].concept_class_id.unique())\n",
    "concept_classes.remove('Disorder')\n",
    "concept_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 11 terms to the graph for concept class Substance.\n",
      "\n",
      "Adding 145 terms to the graph for concept class ['Clinical Finding', 'Disorder'].\n",
      "\n",
      "Adding 45 terms to the graph for concept class Procedure.\n",
      "\n",
      "Adding 9 terms to the graph for concept class Observable Entity.\n",
      "\n",
      "Adding 2 terms to the graph for concept class Context-dependent.\n",
      "\n",
      "Adding 8 terms to the graph for concept class Morph Abnormality.\n",
      "\n",
      "Adding 3 terms to the graph for concept class Social Context.\n",
      "\n",
      "Adding 1 terms to the graph for concept class Event.\n",
      "\n",
      "Total number of terms in the Snomed DAG are 224 with 224 edges.\n"
     ]
    }
   ],
   "source": [
    "full_graph = nx.DiGraph()\n",
    "\n",
    "for concept_class in concept_classes:\n",
    "\n",
    "    # Group clinical finding and disorder together\n",
    "    if concept_class=='Clinical Finding':\n",
    "        concept_class = ['Clinical Finding', 'Disorder']\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&(concept_df.concept_class_id.isin(concept_class))].concept_id)\n",
    "\n",
    "    else:\n",
    "        filtered_terms = set(concept_df[concept_df.concept_id.isin(my_terms)&(concept_df.concept_class_id==concept_class)].concept_id)\n",
    "\n",
    "    G_x = create_SNOMED_CT_graph_based_on_terms(filtered_terms, concept_df, concept_relationship_df, selected_relationship_ids)\n",
    "    \n",
    "    hierarchy_x = build_hierarchy(filtered_terms, G_x, compute_lcas_and_distances, find_lca_and_distance)\n",
    "    graph_x = hierarchy_to_graph(hierarchy_x, filtered_terms)\n",
    "    pruned_graph_x = prune_redundant_edges(graph_x)\n",
    "    graph_with_names_x = replace_nodes_with_names(pruned_graph_x, code_to_name)\n",
    "    \n",
    "    full_graph = nx.compose(full_graph, graph_with_names_x)\n",
    "    print(f'Adding {graph_with_names_x.number_of_nodes()} terms to the graph for concept class {concept_class}.\\n')\n",
    "print(f'Total number of terms in the Snomed DAG are {full_graph.number_of_nodes()} with {full_graph.number_of_edges()} edges.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(full_graph, 'snomed_alternative_grouping_2.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: ['Substance', 'Observable entity', 'Death', 'Procedure', 'FH: Cardiovascular disease', 'Clinical finding', 'Morphologically abnormal structure', 'Social context']\n",
      "Iteration 1: ['Hypercoagulability state', 'Bleeding', 'Patient position finding', 'Pregnancy', 'Immunosuppression', 'Pain', 'Mechanical lesion', 'Increased body mass index', 'Surgical procedure', 'Body temperature above reference range', 'Risk of cardiovascular disease', 'Measurement finding', 'Disability', 'Dysfunction of urinary bladder', 'Leuko-araiosis', 'Aphasia', 'Procedure with a clinical finding focus', 'Able to cope', 'Vascular sclerosis', 'Cardiac syndrome X', 'Glomerular filtration rate', 'Dietary finding', 'Social worker involved', 'Activities of daily living assessment', 'Blood-brain barrier', 'Functional finding', 'Age factor', 'Procedure by method', 'Economic status', 'Catheter procedure', 'General clinical state', 'Weight loss', 'Racial group', 'Physical fitness state', 'Lesion size', 'Drug or medicament', 'Disease', 'DNA damage', 'Family history of stroke', 'Preventive procedure', 'Regimes and therapies', 'Decreased alcohol consumption', 'Finding of tobacco use and exposure', 'Kidney disease', 'Prediabetes', 'Falls', 'Lesion of brain', 'Protein', 'Procedure on artery', 'Paralysis', 'Uses oral contraception', 'Admission to stroke unit', 'Education and/or schooling finding', 'Stress', 'Finding of lesion', 'Current drinker', 'Genetic predisposition', 'Male', 'Social isolation', 'Finding by site']\n",
      "Iteration 2: ['Troponin', 'Introduction procedure', 'Complete obstruction', 'C-reactive protein above reference range', 'Epilepsy due to cerebrovascular accident', 'Malignant neoplastic disease', 'Removal of thrombus', 'Impaired cognition', 'Healthy diet', 'Smoking cessation therapy', 'Disorder of glucose metabolism', 'Passive smoker', 'Decreased vascular flow', 'Cognitive skills training', 'Assessment using assessment scale', 'Secondary prevention', 'Heavy drinker', 'Finding of substance level', 'Delirium', 'Nutritional disorder', 'Arteriosclerosis', 'Finding related to ability to mobilize', 'Primary prevention', 'Percutaneous transluminal procedure on blood vessel', 'Stenosis', 'Continuous positive airway pressure ventilation treatment', 'General finding of soft tissue', 'Depressive disorder', 'Necrosis of anatomical site', 'Low sodium diet', 'Functionally dependent', 'Urinary incontinence', 'Functional finding', 'Insertion of carotid artery stent', 'High carbohydrate diet', 'Endarterectomy', 'Metabolic syndrome X', 'Dyssomnia', 'Moderate drinker', 'Sequelae of neurological disorders', 'Thrombolysis of artery', 'Antidiabetic agent', 'Postprocedural recovery status', 'N-terminal pro-B-type natriuretic peptide', 'Congenital malformation', 'Interleukin-6', 'Speech therapy', 'Disease', 'Finding of renal function', 'Inflammatory disorder', 'Complicated atheromatous plaque', 'Aspiration', 'Care regime', 'Obesity', 'Seizure', 'Smoker', 'Infectious disease', 'Procedure involving urinary catheter', 'Pain management', 'Dyslipidemia', 'Blood glucose management', 'Psychotherapy', 'Activity exercise pattern', 'Quality of life satisfaction', 'Angioplasty of carotid artery', 'High fat diet', 'Disorder by body site']\n",
      "Iteration 3: ['COVID-19', 'Carotid endarterectomy', 'Exercises regularly', 'Thiazolidinedione', 'Disorder of glucose metabolism', 'Systemic inflammatory response syndrome', 'Assessment using Modified Rankin Scale', 'Cerebral infarction', 'Gets no exercise', 'Spasticity', 'Physical therapy procedure', 'Able to mobilize', 'Cachexia', 'Sodium glucose cotransporter subtype 2 inhibitor', 'Disorder of soft tissue', 'Injection of botulinum toxin', 'General finding of soft tissue', 'Catecholamine level - finding', 'Disorder of cardiac function', 'Lipid above reference range', 'Atherosclerosis', 'Ischemic stroke', 'Urinary tract infectious disease', 'Inflammatory disorder', 'Glucagon-like peptide 1 receptor agonist', 'Finding of oxygen saturation', 'Eclampsia', 'Acute infective endocarditis', 'Impaired mobility', 'Chronic kidney disease', 'Cardiac troponin T', 'Mediterranean diet', 'Disorder of body system', 'Inflammation of specific body structures or tissue', 'Cerebral atrophy', 'Organ or tissue vascular perfusion decreased', 'Finding of blood substance level', 'Administration of medication', 'Assessment using National Institutes of Health stroke scale', 'Myocardial infarction']\n",
      "Iteration 4: ['Lipoprotein (a) hyperlipoproteinemia', 'Disorder of cardiovascular system', 'Encephalitis', 'Drug therapy', 'Diabetes mellitus', 'Disorder of soft tissue', 'Pneumonia', 'Disorder of endocrine system', 'Heart failure', 'Myocardial infarction due to atherothrombotic coronary artery disease', 'Myocardial infarction due to demand ischemia', 'Fracture of bone', 'Soft tissue lesion', 'Dysphagia', 'Hyperlipidemia', 'Lacunar infarction', 'Blood glucose level - finding', 'Female sex hormones - serum level - finding', 'Sleep apnea', 'Chemotherapy']\n",
      "Iteration 5: ['Acute heart failure', 'Transient cerebral ischemia', 'Heart disease', 'Low blood pressure', 'Antidepressant therapy', 'Platelet aggregation inhibitor therapy', 'Administration of prophylactic statin', 'Anticoagulant therapy', 'Gestational diabetes mellitus complicating pregnancy', 'Chronic heart failure', 'Antihypertensive therapy', 'Thrombosis', 'Insulin resistance', 'Dissection of cerebral artery', 'Hypertensive disorder', 'Soft tissue lesion', 'Vascular disorder', 'Antibiotic therapy', 'Hypercholesterolemia']\n",
      "Iteration 6: ['Cardiac arrhythmia', 'Pulmonary embolism', 'Thrombus of cardiac chamber', 'Endocarditis', 'Silent micro-hemorrhage of brain', 'Migraine', 'Patent foramen ovale', 'Embolism', 'Dissection of artery', 'Deep venous thrombosis', 'Takotsubo cardiomyopathy', 'Hypertension complicating pregnancy', 'Cerebral venous sinus thrombosis', 'Vasculitis', 'Pre-eclampsia']\n",
      "Iteration 7: ['Migraine with aura', 'Atrial fibrillation']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def synchronized_expansion_with_reappearance(full_graph):\n",
    "    \"\"\"\n",
    "    Perform synchronized hierarchical expansion on a DAG,\n",
    "    keeping nodes active and visible in every iteration until all their children are expanded.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_graph: A NetworkX DiGraph object representing the DAG.\n",
    "    \n",
    "    Returns:\n",
    "    - iterations: A list of lists, where each inner list contains the nodes expanded at that iteration.\n",
    "    \"\"\"\n",
    "    # Step 1: Track parent dependencies (number of parents yet to be processed)\n",
    "    parent_dependencies = {node: full_graph.in_degree(node) for node in full_graph.nodes}\n",
    "\n",
    "    # Step 2: Identify root nodes (those with no incoming edges)\n",
    "    roots = [node for node, indegree in parent_dependencies.items() if indegree == 0]\n",
    "\n",
    "    # Step 3: Iterative expansion with multiple parent handling\n",
    "    processed = set()  # Set to track processed nodes\n",
    "    iterations = []  # List to store nodes expanded at each iteration\n",
    "    current_level = set(roots)  # Start with root nodes\n",
    "\n",
    "    while current_level:\n",
    "        iterations.append(list(current_level))  # Add the current level to the result\n",
    "        next_level = set(current_level)  # Include current nodes by default\n",
    "\n",
    "        for node in current_level:\n",
    "            # Add children to next level if all their parents are processed\n",
    "            for child in full_graph.successors(node):\n",
    "                if child not in processed:\n",
    "                    # Decrement the dependency counter for each child\n",
    "                    parent_dependencies[child] -= 1\n",
    "                    # If all parents are processed, the child becomes eligible\n",
    "                    if parent_dependencies[child] == 0:\n",
    "                        next_level.add(child)\n",
    "\n",
    "            # Only mark the node as processed if all its children are processed\n",
    "            if all(\n",
    "                child in processed or child in next_level\n",
    "                for child in full_graph.successors(node)\n",
    "            ):\n",
    "                processed.add(node)  # Mark the node as fully processed\n",
    "\n",
    "        # Filter out fully processed nodes from the current level\n",
    "        current_level = {node for node in next_level if node not in processed}\n",
    "\n",
    "    return iterations\n",
    "\n",
    "# Example usage\n",
    "iterations = synchronized_expansion_with_reappearance(full_graph)\n",
    "for i, level in enumerate(iterations):\n",
    "    print(f\"Iteration {i}: {level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_depths(graph):\n",
    "    \"\"\"\n",
    "    Precompute the depths (max depth to leaf) for all nodes in the graph.\n",
    "    \"\"\"\n",
    "    depths = {}\n",
    "\n",
    "    def compute_depth(node):\n",
    "        if node in depths:\n",
    "            return depths[node]\n",
    "        if graph.out_degree(node) == 0:  # Leaf node\n",
    "            depths[node] = 0\n",
    "        else:\n",
    "            depths[node] = 1 + max(compute_depth(child) for child in graph.successors(node))\n",
    "        return depths[node]\n",
    "    \n",
    "    for node in nx.topological_sort(graph):\n",
    "        compute_depth(node)\n",
    "    \n",
    "    return depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leo\\miniconda3\\envs\\wise-dag-ml\\lib\\site-packages\\pygraphviz\\agraph.py:1403: RuntimeWarning: Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n",
      "\n",
      "  warnings.warn(b\"\".join(errors).decode(self.encoding), RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bafb03f693479f8b122b9ee2820ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=66), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_reachability_expansion_with_yifan_hu(graph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization for reachability-based expansion of a graph,\n",
    "    using the Yifan Hu layout algorithm for node positioning.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: A NetworkX DiGraph to visualize.\n",
    "    \"\"\"\n",
    "    # Precompute depths for efficiency\n",
    "    depth_cache = precompute_depths(graph)\n",
    "    \n",
    "    def calculate_reachability(node):\n",
    "        \"\"\"Calculate the reachability for a node, which is the depth underneath it and the number of out-degrees (children) it has.\"\"\"\n",
    "        return graph.out_degree(node) + depth_cache[node]\n",
    "\n",
    "    # Identify root nodes (nodes with in-degree 0)\n",
    "    root_nodes = set(node for node in graph.nodes if graph.in_degree(node) == 0)\n",
    "\n",
    "    # Prepare iterations\n",
    "    active_nodes = root_nodes.copy()  # Start with root nodes\n",
    "    processed_nodes = set()\n",
    "    blue_nodes = root_nodes.copy()  # Initially, all root nodes are blue\n",
    "    iterations = []\n",
    "    \n",
    "    while active_nodes:\n",
    "        # Exclude nodes with no children from being highlighted (leaf nodes)\n",
    "        expandable_nodes = {node for node in active_nodes if graph.out_degree(node) > 0}\n",
    "        if not expandable_nodes:\n",
    "            break  # No more nodes to expand\n",
    "        \n",
    "        # Calculate reachability for all expandable nodes\n",
    "        node_reachability = {node: calculate_reachability(node) for node in expandable_nodes}\n",
    "        most_specific_node = max(node_reachability, key=node_reachability.get)\n",
    "        \n",
    "        # Record the current iteration\n",
    "        iterations.append((list(active_nodes), most_specific_node))\n",
    "        \n",
    "        next_level = set(active_nodes)\n",
    "        for node in active_nodes:\n",
    "            if node == most_specific_node:\n",
    "                next_level.remove(node)\n",
    "                for child in graph.successors(node):\n",
    "                    next_level.add(child)\n",
    "                    blue_nodes.add(child)  # Mark children as blue\n",
    "        \n",
    "        # Process the expanded node\n",
    "        processed_nodes.add(most_specific_node)\n",
    "        blue_nodes.discard(most_specific_node)  # Remove the node from blue once it's processed\n",
    "        active_nodes = next_level - processed_nodes\n",
    "\n",
    "    # Generate positions using Yifan Hu layout (requires pygraphviz)\n",
    "    pos = nx.nx_agraph.graphviz_layout(graph, prog='sfdp', args='-Goverlap=false -Gsplines=true')\n",
    "\n",
    "    # Slider callback function\n",
    "    def update(iteration):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        nodes_to_draw, highlighted_node = iterations[iteration]\n",
    "        \n",
    "        # Collect processed nodes up to the current iteration\n",
    "        processed_up_to_now = set(\n",
    "            iterations[i][1] for i in range(iteration)\n",
    "        )\n",
    "        \n",
    "        # Collect blue nodes up to the current iteration\n",
    "        blue_nodes_up_to_now = set(root_nodes)  # Start with root nodes\n",
    "        for i in range(iteration):\n",
    "            _, parent_node = iterations[i]\n",
    "            for child in graph.successors(parent_node):\n",
    "                if child not in processed_up_to_now:\n",
    "                    blue_nodes_up_to_now.add(child)\n",
    "        \n",
    "        # Determine node colors\n",
    "        node_colors = []\n",
    "        for node in graph.nodes:\n",
    "            if node in processed_up_to_now:  # Already expanded nodes\n",
    "                color = 'black'\n",
    "            elif node == highlighted_node:  # Node being split\n",
    "                color = 'red'\n",
    "            elif node in blue_nodes_up_to_now:  # Nodes below the split node and not yet expanded\n",
    "                color = 'blue'\n",
    "            else:  # Unprocessed nodes\n",
    "                color = 'gray'\n",
    "            node_colors.append(color)\n",
    "\n",
    "        # Draw the graph with highlighting\n",
    "        nx.draw(\n",
    "            graph,\n",
    "            pos,\n",
    "            with_labels=True,\n",
    "            node_color=node_colors,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Iteration {iteration}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create the slider\n",
    "    interact(update, iteration=IntSlider(min=0, max=len(iterations) - 1, step=1, value=0))\n",
    "\n",
    "# Usage\n",
    "visualize_reachability_expansion_with_yifan_hu(full_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wise-dag-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
